# dbt Project Setup Guide

This guide walks through setting up and running the dbt project for the Healthcare ETL pipeline.

## Prerequisites

1. Python 3.8 or higher
2. PostgreSQL database (healthcare_warehouse)
3. Silver layer Parquet files generated by the ETL pipeline

## Installation

### 1. Install dbt-postgres

```bash
pip install dbt-postgres dbt-utils
```

### 2. Install dbt packages

```bash
cd dbt_project
dbt deps
```

This will install the `dbt_utils` package defined in `packages.yml`.

## Configuration

### 1. Set Environment Variables

Create a `.env` file or export these variables:

```bash
export POSTGRES_HOST=localhost
export POSTGRES_PORT=5433
export POSTGRES_USER=etl_user
export POSTGRES_PASSWORD=etl_password
export POSTGRES_DB=healthcare_warehouse
export SILVER_DATA_PATH=/path/to/data/silver
```

### 2. Configure dbt Profile

The `profiles.yml` file is already configured to read from environment variables.

For local development, you can also copy `profiles.yml` to `~/.dbt/profiles.yml`:

```bash
mkdir -p ~/.dbt
cp profiles.yml ~/.dbt/profiles.yml
```

### 3. Test Connection

```bash
cd dbt_project
dbt debug
```

This should show all green checkmarks if the configuration is correct.

## Data Loading

Since dbt doesn't natively read Parquet files, we need to load the Silver layer data into PostgreSQL first.

### Load Silver Layer to PostgreSQL

```bash
# Load today's data
python load_silver_to_postgres.py

# Load specific date
python load_silver_to_postgres.py 2025-10-24
```

This script:
- Reads Parquet files from `data/silver/{date}/`
- Creates a `silver` schema in PostgreSQL
- Loads all 9 tables into the `silver` schema
- These tables become the sources for dbt models

## Running dbt

### 1. Run All Models

```bash
cd dbt_project
dbt run
```

This will execute models in the correct order:
1. Staging models (views)
2. Dimension models (tables)
3. Fact models (incremental tables)

### 2. Run Specific Model Groups

```bash
# Run only staging models
dbt run --select staging.*

# Run only dimension models
dbt run --select dimensions.*

# Run only fact models
dbt run --select facts.*

# Run a specific model and its dependencies
dbt run --select +fact_encounter
```

### 3. Run Tests

```bash
# Run all tests
dbt test

# Run tests for specific models
dbt test --select fact_encounter
dbt test --select dim_patient

# Run only custom tests
dbt test --select test_type:singular

# Run only generic tests
dbt test --select test_type:generic
```

### 4. Generate Documentation

```bash
# Generate documentation
dbt docs generate

# Serve documentation locally
dbt docs serve
```

This will open a browser with interactive documentation showing:
- Model lineage (DAG)
- Column descriptions
- Test results
- SQL code

## Incremental Models

Fact tables use incremental materialization for efficiency:

```bash
# Full refresh (rebuild entire table)
dbt run --select fact_encounter --full-refresh

# Incremental run (only new data)
dbt run --select fact_encounter
```

## Development Workflow

### 1. Develop a New Model

```bash
# Create model file
vim models/facts/fact_new_model.sql

# Run the model
dbt run --select fact_new_model

# Test the model
dbt test --select fact_new_model
```

### 2. Modify Existing Model

```bash
# Edit model
vim models/facts/fact_encounter.sql

# Run with full refresh if schema changed
dbt run --select fact_encounter --full-refresh

# Run tests
dbt test --select fact_encounter
```

### 3. Add Tests

Edit the `schema.yml` file in the model directory:

```yaml
models:
  - name: fact_encounter
    columns:
      - name: encounter_id
        tests:
          - unique
          - not_null
```

## Troubleshooting

### Connection Issues

```bash
# Check connection
dbt debug

# Verify environment variables
echo $POSTGRES_HOST
echo $POSTGRES_PORT
```

### Model Failures

```bash
# Run with verbose logging
dbt run --select model_name --debug

# Compile SQL without running
dbt compile --select model_name

# Check compiled SQL
cat target/compiled/healthcare_etl/models/facts/fact_encounter.sql
```

### Test Failures

```bash
# Run specific test
dbt test --select assert_referential_integrity

# Store test failures for inspection
# (configured in dbt_project.yml)
# Query failed tests:
# SELECT * FROM test_results.assert_referential_integrity;
```

### Incremental Model Issues

```bash
# Full refresh to rebuild
dbt run --select fact_encounter --full-refresh

# Check incremental logic
dbt compile --select fact_encounter
```

## Production Deployment

### 1. CI/CD Pipeline

```bash
# In CI/CD pipeline
dbt deps
dbt run --target prod
dbt test --target prod
```

### 2. Scheduling

Use Airflow to schedule dbt runs:

```python
from airflow.operators.bash import BashOperator

dbt_run = BashOperator(
    task_id='dbt_run',
    bash_command='cd /path/to/dbt_project && dbt run --target prod',
    dag=dag
)
```

### 3. Monitoring

Monitor dbt runs:
- Check Airflow logs
- Query dbt artifacts in `target/` directory
- Set up alerts for test failures

## Best Practices

1. **Always run tests after model changes**
   ```bash
   dbt run --select model_name
   dbt test --select model_name
   ```

2. **Use full refresh when schema changes**
   ```bash
   dbt run --select model_name --full-refresh
   ```

3. **Document your models**
   - Add descriptions in `schema.yml`
   - Add comments in SQL files
   - Generate docs regularly

4. **Version control**
   - Commit all model changes
   - Use branches for development
   - Review SQL in pull requests

5. **Test incrementally**
   - Test models as you build them
   - Don't wait until the end

## Additional Resources

- [dbt Documentation](https://docs.getdbt.com/)
- [dbt Best Practices](https://docs.getdbt.com/guides/best-practices)
- [dbt Utils Package](https://github.com/dbt-labs/dbt-utils)
